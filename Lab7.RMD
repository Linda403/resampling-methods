---
title: 'Lab 7: Resampling Methods'
author: "DSO 530"
date: "10/31/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Lab 7: Resampling Methods

You are required to submit two files: a knitted PDF/Word/HTML file that includes your solutions, results (R output), and your comments where needed, and a separate RMD file.

Submit your work via Blackboard.

We used logistic regression to predict the probability of
`default` using `income` and `balance` on the `Default` data set, which is part of the `ISLR2` package. You will now estimate the test error of this logistic regression model using the validation set approach. Do not forget to set a random seed before
beginning your analysis.

@. Fit a logistic regression model that uses `income` and `balance` to
predict `default`.
```{r}
library(ISLR2)
attach(Default)
Default01 <- glm(default~income+balance, data=Default, family= "binomial")
Default01
```


@. Using the validation set approach, estimate the test error of this model. In order to do this, you must perform the following steps:


+ Split the sample set into a training set and a validation set.
```{r}
set.seed(1)
dim(Default)
n=dim(Default)[1]
#n=length(Default[,1])
#length(Default[1,])
k=9000
#the size of the training set. I decided to use 90/10

#train=sample(n, n.train)
index.train=sample(c(1:n), 9000)
#1到n之間select9000個
index.test= -index.train
```

+ Fit a multiple logistic regression model using only the training observations.
```{r}
glm.train <- glm(default~income+balance, data= Default[index.train,], family="binomial")
glm.train
```

+ Obtain a prediction of default status for each individual in
the validation set by computing the posterior probability of
default for that individual, and classifying the individual to
the `default` category if the posterior probability is greater
than 0.5.

Use predict function and specify type="response" option
```{r}
pre <- predict(glm.train, Default[index.test,], type="response")

predicted.values=rep("No", n-k)
predicted.values[pre>0.5]<-'Yes'
#predicted.values
table(predicted.values)
```

+ Compute the validation set error, which is the fraction of
the observations in the validation set that are misclassified.
```{r}
testing_y=Default[index.test,1]
#why we can use this line to get the result of testing data?
#why we don't need to specify 0.5 probablity in testing_y?
#testing_y
table(predicted.values, testing_y)
mean(predicted.values!=testing_y)*100
#a bit confused about the logic we compute the validation set error
#?mean
```

@. Repeat the process in (2) three times, using three different splits
of the observations into a training set and a validation set (you'll need to modify random seed). Comment on the results obtained.
```{r}
set.seed(22)
index.train02=sample(c(1:n), k)
index.test02= -index.train02

glm.train02 <- glm(default~income+balance, data= Default[index.train02,], family="binomial")
#glm.train02

pre02 <- predict(glm.train02, Default[index.test02,], type="response")

predicted.values02=rep("No", n-k)
predicted.values02[pre02>0.5]<-'Yes'
#table(predicted.values02)

testing_y02=Default[index.test02,1]
#testing_y02
mean(predicted.values02!=testing_y02)*100

index.train03=sample(c(1:n), k)
index.test03= -index.train03

glm.train03 <- glm(default~income+balance, data= Default[index.train03,], family="binomial")
#glm.train03

pre03 <- predict(glm.train03, Default[index.test03,], type="response")

predicted.values03=rep("No", n-k)
predicted.values03[pre>0.5]<-'Yes'
#table(predicted.values03)

testing_y03=Default[index.test03,1]
#testing_y03
mean(predicted.values03!=testing_y03)*100

index.train04=sample(c(1:n), k)
index.test04= -index.train04

glm.train04 <- glm(default~income+balance, data= Default[index.train04,], family="binomial")
#glm.train04

pre04 <- predict(glm.train04, Default[index.test04,], type="response")

predicted.values04=rep("No", n-k)
predicted.values04[pre>0.5]<-'Yes'
#table(predicted.values04)

testing_y04=Default[index.test04,1]
#testing_y04
mean(predicted.values04!=testing_y04)*100
```
The first random split gives a misclassification rate of 1.7;
The second random split gives a misclassification rate of 4.9;
The third random split gives a misclassification rate of 3.6

Based on the results, there is a certain degree of variation on the misclassification rate of different random splits, which is reasonable since there may be outliers in certain split. 


@. Now consider a logistic regression model that predicts the probability
of `default` using `income`, `balance`, and a dummy variable
for `student`. Estimate the test error for this model using the validation set approach as in (1)-(2). Comment on whether or not including a dummy variable for `student` leads to a reduction in the test error rate.
```{r}
#glm(default~income+balance+student, data= Default, family="binomial")

set.seed(1)
index.train05=sample(c(1:n), k)
index.test05= -index.train05

glm.train05 <- glm(default~income+balance, data= Default[index.train05,], family="binomial")
#glm.train05

pre05 <- predict(glm.train05, Default[index.test05,], type="response")

predicted.values05=rep("No", n-k)
predicted.values05[pre>0.5]<-'Yes'
#table(predicted.values05)

testing_y05=Default[index.test05,1]
#testing_y05
mean(predicted.values05!=testing_y05)*100
```
After adding the dummy variable "student", the misclassification rate is the same as the model without the variable. This shows that the "student" variable does not have an effect on the predictive power of the model. This is a data-driven way to identify which variable is significant or not rather than a statistical way by looking at t-test. 
